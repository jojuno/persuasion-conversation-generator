{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helper_funcs import report_f1_results, report_accuracy, split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our feature matrices and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_feature_matrix = np.loadtxt(\"./features/bag_of_words.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_feature_matrix = np.loadtxt(\"./features/bigrams.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_matrix = np.loadtxt(\"./features/tfidf.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_matrix = np.loadtxt(\"./features/word2vec_averaged.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.loadtxt(\"./features/labels.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_perceptron_train(features, labels, max_iters=100):\n",
    "    D = len(features[0])\n",
    "    w = np.zeros(shape=(D))                # initialize our weights vector with all 0's\n",
    "    b = 0                                  # initialize our bias as 0\n",
    "\n",
    "    for idx in range(max_iters):\n",
    "        print(f\" Iteration: [{idx + 1} / {max_iters}]\\r\", end='', flush=True)\n",
    "        for x, y in zip(features, labels):\n",
    "            a = np.dot(w, x) + b        # compute the activation for the example\n",
    "            if y * a <= 0:\n",
    "                if y != 1: x = x * y\n",
    "                w = np.add(w, x)        # update our weights\n",
    "                b = b + y               # update our bias\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Return our weights and biases\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaged Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_perceptron_train(features, labels, max_iters=100):\n",
    "    D = len(features[0])\n",
    "    ctr = 1\n",
    "\n",
    "    w = np.zeros(shape=(D))                # initialize our weights vector with all 0's\n",
    "    b = 0                                  # initialize our bias as 0\n",
    "    w_cached = np.zeros(shape=(D))         # initialize our cached weights vector with all 0's\n",
    "    b_cached = 0                           # initialize our cached bias as 0\n",
    "\n",
    "    for idx in range(max_iters):\n",
    "        print(f\" Iteration: [{idx + 1} / {max_iters}]\\r\", end='', flush=True)\n",
    "        for x, y in zip(features, labels):\n",
    "            # Go through our truthfulness calculations\n",
    "            if y * (np.dot(w, x) + b) <= 0:\n",
    "                w = np.add(w, (y * x))                        # Update our weights\n",
    "                b = b + y                                     # Update our bias\n",
    "                w_cached = np.add(w_cached, (y * ctr * x))    # Update our cached weights\n",
    "                b_cached = b_cached + (y * ctr)               # Update our cached bias\n",
    "\n",
    "            ctr = ctr + 1\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Return the averaged weights biases biases\n",
    "    inverse_ctr = 1 / ctr     \n",
    "    return w - (inverse_ctr * w_cached), b - (inverse_ctr * b_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_perceptron_train(features, labels):\n",
    "    max_iter = 100000\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    perceptron = Perceptron(max_iter=max_iter,\n",
    "                            eta0=learning_rate)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    labels = labels.astype('int')\n",
    "    perceptron.fit(features, labels)\n",
    "\n",
    "    # Return the trained model\n",
    "    return perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_perceptron_test(features, labels, model, feature_type=\"No Feature Type Provided\"):\n",
    "    label_predictions = model.predict(features)\n",
    "\n",
    "    # precision = precision_score(list(labels), label_predictions, average=None)\n",
    "    # recall = recall_score(list(labels), label_predictions, average=None)\n",
    "    # f1 = f1_score(list(labels), label_predictions, average=None)\n",
    "    accuracy = accuracy_score(list(labels), label_predictions)\n",
    "\n",
    "    # report_f1_results(precision, recall, f1, feature_type, \"sklearn Perceptron\")\n",
    "    report_accuracy(accuracy, feature_type, \"sklearn Perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_svm_train(features, labels):\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(features, labels)\n",
    "\n",
    "    # Return the trained model\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_svm_test(features, labels, model, feature_type=\"No Feature Type Provided\"):\n",
    "    label_predictions = model.predict(features)\n",
    "\n",
    "    # precision = precision_score(list(labels), label_predictions, average=None)\n",
    "    # recall = recall_score(list(labels), label_predictions, average=None)\n",
    "    # f1 = f1_score(list(labels), label_predictions, average=None)\n",
    "    accuracy = accuracy_score(list(labels), label_predictions)\n",
    "\n",
    "    # report_f1_results(precision, recall, f1, feature_type, \"sklearn SVM\")\n",
    "    report_accuracy(accuracy, feature_type, \"sklearn SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create our Testing and Training classes that inherit from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        # Load in our utterances and clasifications as tensors\n",
    "        self.x = torch.from_numpy(features.astype(np.float32))\n",
    "        self.y = torch.from_numpy(labels.astype(np.int8))\n",
    "        self.n_samples = self.x.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the feature vector and the label\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        # Load in our utterances and clasifications as tensors\n",
    "        self.x = torch.from_numpy(features.astype(np.float32))\n",
    "        self.y = torch.from_numpy(labels.astype(np.int8))\n",
    "        self.n_samples = self.x.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the feature vector and the label\n",
    "        return self.x[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input_nodes, n_output_nodes=len(list(set(labels)))):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        n_hidden_nodes_1 = 50\n",
    "        n_hidden_nodes_2 = 10\n",
    "\n",
    "        # (input -> hidden_1)\n",
    "        self.fc1 = nn.Linear(n_input_nodes, n_hidden_nodes_1)\n",
    "\n",
    "        # (hidden_1 -> hidden_2)\n",
    "        self.fc2 = nn.Linear(n_hidden_nodes_1, n_hidden_nodes_2)\n",
    "\n",
    "        # (hidden_2 -> output)\n",
    "        self.fc3 = nn.Linear(n_hidden_nodes_2, n_output_nodes)\n",
    "\n",
    "        # dropout layer to avoid overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer with ReLU activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # add hidden layer with ReLU activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fnn(model, train_loader, valid_loader):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Number of epochs\n",
    "    n_epochs = 50\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            # target = target.squeeze(1)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data.float())\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_function(output, target.long())\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # Validate the model #\n",
    "        model.eval() # prep the model for evaluation\n",
    "        for data, target in valid_loader:\n",
    "            # target = target.squeeze(1)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data.float())\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_function(output, target.long())\n",
    "\n",
    "            # update running validation loss\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), './models/FNN_model.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fnn(labels, model, dataloader, feature_type=\"No Feature Type Provided\"):\n",
    "    prediction_list = []\n",
    "    for _, batch in enumerate(dataloader):\n",
    "        outputs = model(batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(predicted.cpu())\n",
    "\n",
    "    # Calculate and report accuracy\n",
    "    n = len(predicted)\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        act = labels[i]\n",
    "        if act == predicted[i]: correct += 1\n",
    "\n",
    "    accuracy = correct / n\n",
    "    report_accuracy(accuracy, feature_type, \"torch FNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into testing and training subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_x, bow_test_x, bow_train_y, bow_test_y = split_data(bow_feature_matrix, labels)\n",
    "bigram_train_x, bigram_test_x, bigram_train_y, bigram_test_y = split_data(bigram_feature_matrix, labels)\n",
    "tfidf_train_x, tfidf_test_x, tfidf_train_y, tfidf_test_y = split_data(tfidf_feature_matrix, labels)\n",
    "w2v_train_x, w2v_test_x, w2v_train_y, w2v_test_y = split_data(w2v_feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the sklearn Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train the model using all the different feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = sklearn_perceptron_train(bow_train_x, bow_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = sklearn_perceptron_train(bigram_train_x, bigram_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = sklearn_perceptron_train(tfidf_train_x, tfidf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = sklearn_perceptron_train(w2v_train_x, w2v_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the model using all the different feature matrices and compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bag of Words]\t[sklearn Perceptron]\t\tAccuracy: 46.93%\n",
      "[Bigram]\t[sklearn Perceptron]\t\tAccuracy: 48.26%\n",
      "[TF-IDF]\t[sklearn Perceptron]\t\tAccuracy: 44.85%\n",
      "[Word2Vec]\t[sklearn Perceptron]\t\tAccuracy: 44.35%\n"
     ]
    }
   ],
   "source": [
    "sklearn_perceptron_test(bow_test_x, bow_test_y, bow_model, \"Bag of Words\")\n",
    "sklearn_perceptron_test(bigram_test_x, bigram_test_y, bigram_model, \"Bigram\")\n",
    "sklearn_perceptron_test(tfidf_test_x, tfidf_test_y, tfidf_model, \"TF-IDF\")\n",
    "sklearn_perceptron_test(w2v_test_x, w2v_test_y, w2v_model, \"Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the sklearn SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train the model using all the different feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = sklearn_svm_train(bow_train_x, bow_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brent\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "bigram_model = sklearn_svm_train(bigram_train_x, bigram_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = sklearn_svm_train(tfidf_train_x, tfidf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = sklearn_svm_train(w2v_train_x, w2v_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the model using all the different feature matrices and compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bag of Words]\t[sklearn SVM]\t\tAccuracy: 52.99%\n",
      "[Bigram]\t[sklearn SVM]\t\tAccuracy: 51.99%\n",
      "[TF-IDF]\t[sklearn SVM]\t\tAccuracy: 55.32%\n",
      "[Word2Vec]\t[sklearn SVM]\t\tAccuracy: 54.57%\n"
     ]
    }
   ],
   "source": [
    "sklearn_svm_test(bow_test_x, bow_test_y, bow_model, \"Bag of Words\")\n",
    "sklearn_svm_test(bigram_test_x, bigram_test_y, bigram_model, \"Bigram\")\n",
    "sklearn_svm_test(tfidf_test_x, tfidf_test_y, tfidf_model, \"TF-IDF\")\n",
    "sklearn_svm_test(w2v_test_x, w2v_test_y, w2v_model, \"Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the torch Feedforward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 0       # Number of subprocesses to use for data loading\n",
    "batch_size = 50     # How many samples per batch to load\n",
    "valid_size = 0.2    # Percentage of training set to use as validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train the model using all the different feature matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put our training data/labels into a DataFrame\n",
    "bow_train_data = TrainDataset(bow_train_x, bow_train_y)\n",
    "bow_test_data = TestDataset(bow_test_x, bow_test_y)\n",
    "\n",
    "# Obtain training indices that will be used for validation\n",
    "num_train = len(bow_train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "bow_train_idx, bow_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define samplers for obtaining training and validation batches\n",
    "bow_train_sampler = SubsetRandomSampler(bow_train_idx)\n",
    "bow_valid_sampler = SubsetRandomSampler(bow_valid_idx)\n",
    "\n",
    "# Prepare data loaders\n",
    "bow_train_loader = DataLoader(dataset=bow_train_data, batch_size=batch_size, sampler=bow_train_sampler, num_workers=n_workers)\n",
    "bow_valid_loader = DataLoader(dataset=bow_train_data, batch_size=batch_size, sampler=bow_valid_sampler, num_workers=n_workers)\n",
    "bow_test_loader = DataLoader(dataset=bow_test_data, batch_size=batch_size, num_workers=n_workers)\n",
    "\n",
    "# Create and train our FNN model using the BoW feature vectors\n",
    "bow_model = Net(bow_feature_matrix.shape[1])\n",
    "train_fnn(bow_model, bow_train_loader, bow_valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Bigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put our training data/labels into a DataFrame\n",
    "bigram_train_data = TrainDataset(bigram_train_x, bigram_train_y)\n",
    "bigram_test_data = TestDataset(bigram_test_x, bigram_test_y)\n",
    "\n",
    "# Obtain training indices that will be used for validation\n",
    "num_train = len(bigram_train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "bigram_train_idx, bigram_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define samplers for obtaining training and validation batches\n",
    "bigram_train_sampler = SubsetRandomSampler(bigram_train_idx)\n",
    "bigram_valid_sampler = SubsetRandomSampler(bigram_valid_idx)\n",
    "\n",
    "# Prepare data loaders\n",
    "bigram_train_loader = DataLoader(dataset=bigram_train_data, batch_size=batch_size, sampler=bigram_train_sampler, num_workers=n_workers)\n",
    "bigram_valid_loader = DataLoader(dataset=bigram_train_data, batch_size=batch_size, sampler=bigram_valid_sampler, num_workers=n_workers)\n",
    "bigram_test_loader = DataLoader(dataset=bigram_test_data, batch_size=batch_size, num_workers=n_workers)\n",
    "\n",
    "# Create and train our FNN model using the Bigram feature vectors\n",
    "bigram_model = Net(bigram_feature_matrix.shape[1])\n",
    "train_fnn(bigram_model, bigram_train_loader, bigram_valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put our training data/labels into a DataFrame\n",
    "tfidf_train_data = TrainDataset(tfidf_train_x, tfidf_train_y)\n",
    "tfidf_test_data = TestDataset(tfidf_test_x, tfidf_test_y)\n",
    "\n",
    "# Obtain training indices that will be used for validation\n",
    "num_train = len(tfidf_train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "tfidf_train_idx, tfidf_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define samplers for obtaining training and validation batches\n",
    "tfidf_train_sampler = SubsetRandomSampler(tfidf_train_idx)\n",
    "tfidf_valid_sampler = SubsetRandomSampler(tfidf_valid_idx)\n",
    "\n",
    "# Prepare data loaders\n",
    "tfidf_train_loader = DataLoader(dataset=tfidf_train_data, batch_size=batch_size, sampler=tfidf_train_sampler, num_workers=n_workers)\n",
    "tfidf_valid_loader = DataLoader(dataset=tfidf_train_data, batch_size=batch_size, sampler=tfidf_valid_sampler, num_workers=n_workers)\n",
    "tfidf_test_loader = DataLoader(dataset=tfidf_test_data, batch_size=batch_size, num_workers=n_workers)\n",
    "\n",
    "# Create and train our FNN model using the TF-IDF feature vectors\n",
    "tfidf_model = Net(tfidf_feature_matrix.shape[1])\n",
    "train_fnn(tfidf_model, tfidf_train_loader, tfidf_valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put our training data/labels into a DataFrame\n",
    "w2v_train_data = TrainDataset(w2v_train_x, w2v_train_y)\n",
    "w2v_test_data = TestDataset(w2v_test_x, w2v_test_y)\n",
    "\n",
    "# Obtain training indices that will be used for validation\n",
    "num_train = len(w2v_train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "w2v_train_idx, w2v_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define samplers for obtaining training and validation batches\n",
    "w2v_train_sampler = SubsetRandomSampler(w2v_train_idx)\n",
    "w2v_valid_sampler = SubsetRandomSampler(w2v_valid_idx)\n",
    "\n",
    "# Prepare data loaders\n",
    "w2v_train_loader = DataLoader(dataset=w2v_train_data, batch_size=batch_size, sampler=w2v_train_sampler, num_workers=n_workers)\n",
    "w2v_valid_loader = DataLoader(dataset=w2v_train_data, batch_size=batch_size, sampler=w2v_valid_sampler, num_workers=n_workers)\n",
    "w2v_test_loader = DataLoader(dataset=w2v_test_data, batch_size=batch_size, num_workers=n_workers)\n",
    "\n",
    "# Create and train our FNN model using the Word2Vec feature vectors\n",
    "w2v_model = Net(w2v_feature_matrix.shape[1])\n",
    "train_fnn(w2v_model, w2v_train_loader, w2v_valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the model using all the different feature matrices and compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bag of Words]\t[torch FNN]\t\tAccuracy: 0.00%\n",
      "[Bigram]\t[torch FNN]\t\tAccuracy: 25.00%\n",
      "[TF-IDF]\t[torch FNN]\t\tAccuracy: 25.00%\n",
      "[Word2Vec]\t[torch FNN]\t\tAccuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "test_fnn(bow_test_y, bow_model, bow_test_loader, feature_type=\"Bag of Words\")\n",
    "test_fnn(bigram_test_y, bigram_model, bigram_test_loader, feature_type=\"Bigram\")\n",
    "test_fnn(tfidf_test_y, tfidf_model, tfidf_test_loader, feature_type=\"TF-IDF\")\n",
    "test_fnn(w2v_test_y, w2v_model, w2v_test_loader, feature_type=\"Word2Vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb69f1298249730a38e9c908db43e5574824b46301e1be803b6723b7b558f467"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
