{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the dialogue and info dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df = pd.read_csv('dialogue_df_cleaned.csv')\n",
    "info_df = pd.read_csv('info_df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the dataframes for the persuader's utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuader_df = dialogue_df[dialogue_df.role == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the labels array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_styles = list(persuader_df.er_label_1.unique())\n",
    "persuader_styles_map = {style: idx for idx, style in enumerate(persuasion_styles)} # Generate a style: idx map so we can easily map styles to integers\n",
    "\n",
    "labels = np.array([persuader_styles_map[style] for style in persuader_df.er_label_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the labels array to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./features/labels.out\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use sklearn to get the TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# N x M matrix that holds our tfidf features. N = # of persuader utterances and M = # of unique words in all persuader utterances\n",
    "tfidf_feature_matrix = vectorizer.fit_transform(persuader_df['sentence'])\n",
    "tfidf_feature_matrix = tfidf_feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write the TF-IDF feature matrix to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./features/tfidf.out\", tfidf_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use gensim to load the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Capture the Word2Vec features and use the AVERAGE as the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our N x M variables\n",
    "N = len(persuader_df[\"sentence\"])\n",
    "M = len(wv[0])\n",
    "\n",
    "# Create an empty N x M numpy array to represent our feature matrix\n",
    "wv_avg_feature_matrix = np.empty(shape=(N, M))\n",
    "\n",
    "# Iterate through each utterance in the persuader dataframe\n",
    "for i, utterance in enumerate(persuader_df[\"sentence\"]):\n",
    "    # Store the Word2Vec vector for each valid word in the review\n",
    "    vec_list = [wv[word] for word in utterance.split() if word in wv]\n",
    "\n",
    "    # Create a single feature vector by averaging together each vector from each word\n",
    "    n = len(vec_list) if len(vec_list) > 0 else 1\n",
    "    vec = sum(vec_list) / n\n",
    "\n",
    "    # Append our feature to the feature matrix\n",
    "    wv_avg_feature_matrix[i] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the averaged Word2Vec feature matrix to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./features/word2vec_averaged.out\", wv_avg_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Capture the Word2Vec features and use the FIRST 10 VECTORS as the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This feature matrix has terrible performancem, we can probably just ignore this\n",
    "\n",
    "C = 10\n",
    "\n",
    "# Create an empty N x M x 10 numpy array to represent our feature matrix\n",
    "wv_ten_feature_matrix = np.empty([N, M * C])\n",
    "\n",
    "# Iterate through each review in the DataFrame\n",
    "for i, utterance in enumerate(persuader_df[\"sentence\"]):\n",
    "    # Store the Word2Vec vector for the first 10 valid words in the review\n",
    "    vec_list = np.empty([M * C])\n",
    "    ctr = 0\n",
    "    for word in utterance.split():\n",
    "        if ctr >= M * C: break\n",
    "        if word not in wv: continue\n",
    "        for val in wv[word]: \n",
    "            vec_list[ctr] = val\n",
    "            ctr += 1\n",
    "\n",
    "    # Append our feature to the feature matrix\n",
    "    wv_ten_feature_matrix[i] = vec_list\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write the first ten Word2Vec feature matrix to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"./features/word2vec_first_ten.out\", wv_ten_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a list of unique words in all persuader utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set([word for utterance in persuader_df[\"sentence\"] for word in utterance.split()]))\n",
    "unique_words_dict = {word: idx for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use the unique words to create our BoW feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_feature(utterance):\n",
    "    feature = [0] * len(unique_words)\n",
    "    for word in utterance.split():\n",
    "        idx = unique_words_dict[word] if word in unique_words_dict else None\n",
    "        if idx is not None:\n",
    "            feature[idx] += 1\n",
    "\n",
    "    return np.array(feature)\n",
    "\n",
    "bow_feature_matrix = np.array([generate_bow_feature(utterance) for utterance in persuader_df[\"sentence\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the BoW feature matrix to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./features/bag_of_words.out\", bow_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate a one-hot encoding of all bigrams in the utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [bigram for utterance in persuader_df[\"sentence\"] for bigram in zip(utterance.split()[:-1], utterance.split()[1:])]\n",
    "bigrams_dict = {bigram: idx for idx, bigram in enumerate(bigrams)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert the utterances to a bigram feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram_feature(utterance):\n",
    "    feature = [0] * len(bigrams)\n",
    "\n",
    "    utterance = utterance.split()\n",
    "    for i in range(len(utterance) - 1):\n",
    "        bigram = (utterance[i], utterance[i + 1])\n",
    "        bigram_idx = bigrams_dict[bigram] if bigram in bigrams_dict else None\n",
    "        if bigram_idx is not None:\n",
    "            feature[bigram_idx] += 1\n",
    "\n",
    "    return np.array(feature)\n",
    "\n",
    "bigram_feature_matrix = np.array([generate_bigram_feature(utterance) for utterance in persuader_df[\"sentence\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the Bigrams feature matrix to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./features/bigrams.out\", bow_feature_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb69f1298249730a38e9c908db43e5574824b46301e1be803b6723b7b558f467"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
